---
title: About Hayley
permalink: /about/
layout: single
---

I'm a third year PhD student in the Department of Linguistics at Harvard University with a dream of combining theoretical semantics with computational linguistics. 

I have a deep love for theory and formalisms, with a soft spot for lambda calculus, fancy operators and continuation semantics. My background before coming to linguistics was in mathematics (specifically logic and set theory) and computer science.
I'm currently pursuing projects in theoretical and experimental semantics on anaphora and indefinites, specifically on crossover and the handling of indefinites and discourse referents. I've been working on series of experiments to lay the groundwork for wh-crossover and quantificational crossover &ndash; there's actually surprisingly little experimental data on crossover. With these results in hand, the plan is to compare how proper names and indefinites behave in the same situations, as well as in other non-c-command configurations such as _when_-clauses and conjunction. Proper names, at least, seem to show a similar crossover effect when used with cataphora, suggesting that some mechanism relating coference and binding (such as Heim's Rule I) is indeed needed.

I'm always interested in the debate of whether we need linguistics for natural language processing. I would argue that we do! Linguistics lets us understand the strengths and limitations of large neural language models by giving us a well-understood set of phenomena to measure language models by, and can help us pinpoint ways in which language models are not, in fact, modelling human language the way we do. Moreover, linguistic typology gives us a principled way to structure cross-lingual transfer learning. I'm also very interested in the question of whether NLP and language acquisition are converging &ndash; whether the trending "more data and bigger models" approach to NLP will ever produce a human-like approach to language, or whether we will turn back to building hybrid systems which enforce syntactic, semantic or pragmatic structure in order to learn language with the mechanisms children use to learn, with vastly less data than their neural network counterparts. Is it possible to learn human-like language without being immersed in a human-like world?

In the meantime, I'm deeply interested in what modern language models understand about language. We know that models like GPT-3 or T5 can tell plausible stories, provide convincing explanations of both facts and jokes and write short essays at the level of the average high-schooler. But does reading the entirety of the internet really teach them all the subtle rules of language that humans subsconsciously apply, or are they just able to parrot common constructions and snippets of sentences? What does it mean for a language model to "know" or "understand" language? How do you tell whether a language model "understands" the meaning of a sentence? 
In particular, I'm interested in whether language models capture the compositionality of meaning: humans effortlessly understand sentences they've never seen before by putting together the individual words and phrases in a systematic way. Verbs combine with their arguments, adjectives combine with nouns, and so on. Do language models do this?
I'm currently collaborating with Jacob Andreas, Anna Ivanova and a group of researchers at MIT across computer science and cognitive science to look into meaning. My current work focuses on adjective composition, and our goal as a group is to build up a broad dataset capturing what language models understand about meaning.

When I'm not doing research, you can find me reading, hiking (ideally in the Swiss Alps!) and spoiling my fluffy black cat, Aurora. If you ever need a native Swiss German (or British English) speaker for your research, let me know.
