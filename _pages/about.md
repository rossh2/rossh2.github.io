---
title: About Hayley
permalink: /about/
layout: single
---

I'm a fourth year PhD student (as of September 2023) studying Linguistics at Harvard University, with a focus on computational linguistics and semantics.

Iâ€™m interested in how we can use linguistics and experimental data to pin down language models' capabilities and limitations and develop ways to improve them. 
Language models like ChatGPT have taken the world by storm, but we're still far from understanding how they actually achieve such impressive performance on a wide range of real-life tasks &endash; and why they still fail to handle important aspects of language meaning ranging from seemingly [simple](http://arxiv.org/abs/2306.08189) [negation](https://aclanthology.org/2022.coling-1.272) to more subtle problems like [entity tracking](https://arxiv.org/abs/2305.02363). Studying their behaviour compared to human performance is the first step in identifying where they fall short, what kind of in-built biases they have (or don't have) and how we can improve them. This may involve &ldquo;inoculating&rdquo; their pretraining data with small sets of carefully designed sentences showing the requisite phenomena, fine-tuning, or even building appropriate biases into the network architecture itself. But we won't know what we need to fix until we map out where they fall short.


I'm always interested in the debate of whether we need linguistics for natural language processing. I would argue that we do! Linguistics lets us understand the strengths and limitations of large neural language models by giving us a well-understood set of phenomena to measure language models by, and can help us pinpoint ways in which language models are not, in fact, modelling human language the way we do.
Moreover, linguistic typology gives us a principled way to structure cross-lingual transfer learning. 
I'm also very interested in the question of whether NLP and language acquisition are converging &ndash; whether the trending &ldquo;more data and bigger models&rdquo; approach to NLP will ever produce a human-like approach to language, or whether we will turn back to building hybrid systems which enforce syntactic, semantic or pragmatic structure in order to learn language with something closer to the mechanisms children use to learn, with vastly less data than their neural network counterparts. Is it possible to learn human-like language without being immersed in a human-like world, and using only distributional approaches with no hard-and-fast rules?

In the meantime, I want to understand what modern language models understand about language. We know that models like GPT-3 or T5 can tell plausible stories, provide convincing explanations of both facts and jokes and write short essays at the level of the average high-schooler. But does reading the entirety of the internet really teach them all the subtle rules of language that humans subconsciously apply, or are they just able to parrot common constructions and snippets of sentences? What does it mean for a language model to &ldquo;know&rdquo; or &ldquo;understand&rdquo; language? How do you tell whether a language model &ldquo;understands&rdquo; the meaning of a sentence? 
In particular, I'm interested in whether language models capture the compositionality of
 meaning: humans effortlessly understand sentences they've never seen before by putting 
together the individual words and phrases in a systematic way. I'm currently looking at 
this through the lens of adjective-noun composition, and in particular privativity: how 
some adjective-noun combinations like _counterfeit money_ can result in the new concept being precisely not _money_ while others, like _counterfeit sneakers_, still qualify as _sneakers_. How do humans and language models handle combinations like this that they've never seen before?

When I'm not thinking about language models, I work on experimental semantics, specifically crossover: whether a pronoun can be coconstrued with a referent (quantifier, wh-word, indefinite or proper name) that is hierarchically above it in a sentence. Crossover has been the subject of much theoretical literature and remarkably divided opinions on whether crossover applies to non-standard cases like indefinites or relative clauses. Working with Kathryn Davidson and Gennaro Chierchia, we're developing a [robust experiment design](/assets/publications/Ross-et-al_SuB_Crossover_Proceedings.pdf) to clarify under what circumstances it does and doesn't occur, with several theoretical ramifications for theories of dynamic semantics or situation semantics.
I also like to indulge in reading new theoretical accounts of indefinites and other areas of semantics. Hiding under the experimental facade, I have a deep love for theory and formalisms, with a soft spot for lambda calculus, fancy operators and continuation semantics. (My background before coming to linguistics was in mathematics, specifically logic and set theory, and a little in computer science.)

Outside of my research, you can find me reading, hiking (ideally in the Swiss Alps!) and spoiling my fluffy black cat, Aurora. If you ever need a native Swiss German (or British English) speaker for your research, let me know.

